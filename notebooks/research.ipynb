{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import ave\n",
    "\n",
    "from typing import List, Dict, Union, Callable, Tuple\n",
    "from functools import partial\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "from ave.compat import ActivationAddition, get_x_vector, print_n_comparisons\n",
    "from accelerate import init_empty_weights, load_checkpoint_and_dispatch\n",
    "from functools import lru_cache\n",
    "from ipywidgets import interact, IntSlider, FloatSlider, Text, fixed\n",
    "\n",
    "# from tuned_lens import TunedLens\n",
    "# from tuned_lens.plotting import PredictionTrajectory\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6facf2c62f9749aa8ede3fbe0a49eb6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_path: str = \"../models/llama-13B\"\n",
    "device: str = \"mps\" if torch.has_mps else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "with init_empty_weights():\n",
    "    model = LlamaForCausalLM.from_pretrained(model_path)\n",
    "    model.tie_weights() # in case checkpoint doesn't contain duplicate keys for tied weights\n",
    "\n",
    "# {0: '20G', 1: '20G'}\n",
    "model = load_checkpoint_and_dispatch(model, model_path, device_map={'': device}, dtype=torch.float16, no_split_module_classes=[\"LlamaDecoderLayer\"])\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_path)\n",
    "model.tokenizer = tokenizer\n",
    "_ = torch.set_grad_enabled(False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per-token loss helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0443b493553a45abb4530e034bcb83f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<p><span style=\"color: rgb(162.5442857142857, 92.4557142857143, 0);\">I</span> <span style=\"color: …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import HTML\n",
    "from IPython.display import display\n",
    "\n",
    "def logprobs_html(tokens: List[str], losses: List[float], scale=-255/35, bias=0):\n",
    "    losses = [scale*(l + bias) for l in losses]\n",
    "    # FIXME: Figure out how tokenizer.decode works, don't blindly join on spaces.\n",
    "    return \" \".join([\n",
    "        f'<span style=\"color: rgb({min(l, 255)}, {min(255-l, 255)}, 0);\">{s}</span>'\n",
    "        for s,l in zip(tokens, losses)\n",
    "    ])\n",
    "\n",
    "\n",
    "# Test it\n",
    "prompt = \"I like to eat cheese and crackers\"\n",
    "tokens = tokenizer.batch_decode([[t] for t in tokenizer.encode(prompt)])[1:]\n",
    "losses = [-22.31, -20., -5., -2., -10., -2., -4.]\n",
    "display(HTML(f'<p>{logprobs_html(tokens, losses)}</p>'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore AVE vectors with a perplexity dashboard\n",
    "\n",
    "Features / Ideas for MVP:\n",
    "- [x] Cache things\n",
    "- [x] Show losses relative to baseline of no modification.\n",
    "- [ ] Visualize logprobs per-token\n",
    "- [ ] Make caches accessible, have updating plots for all cached datapoints\n",
    "- [ ] Create comparison texts automatically-ish for an x-vector\n",
    "\n",
    "Additionally nice:\n",
    "- [ ] Show tuned lens of current thing on each text (requires lens for llamas or logit lens)\n",
    "- [ ] Test extracting x-vector from subset of a longer prompt instead of just the \"Love\" vs. \"Hate\" tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(maxsize=1000)\n",
    "def get_diff_vector(prompt_add: str, prompt_sub: str, layer: int):\n",
    "    return ave.get_diff_vector(model, tokenizer, prompt_add, prompt_sub, layer)\n",
    "\n",
    "@lru_cache\n",
    "def run_ave(coeff: float, layer: int, prompt_add: str, prompt_sub: str, texts: Tuple[str]):\n",
    "    # TODO: Could compute act_diff for all layers at once, then a single fwd pass of cost for changing layer.\n",
    "    act_diff = coeff * get_diff_vector(prompt_add, prompt_sub, layer)\n",
    "    blocks = ave.get_blocks(model)\n",
    "    with ave.pre_hooks([(blocks[layer], ave.get_hook_fn(act_diff))]):\n",
    "        inputs = ave.tokenize(tokenizer, list(texts))\n",
    "        output = model(**inputs)\n",
    "\n",
    "    logits = torch.log_softmax(output.logits.to(torch.float32), -1)\n",
    "    token_loss = logits.gather(-1, inputs['input_ids'].unsqueeze(-1)).squeeze(-1)\n",
    "    loss = -token_loss.mean(1) # NOTE: mean fucks with padding tokens a bit, probably fine.\n",
    "\n",
    "    return loss, -token_loss\n",
    "\n",
    "def run_ave_interactive(*args, **kwargs):\n",
    "    split_at = len(kwargs['texts'][0])\n",
    "    kwargs['texts'] = kwargs['texts'][0] + kwargs['texts'][1]\n",
    "    \n",
    "    loss, token_loss = run_ave(*args, **kwargs)\n",
    "    abs_loss, abs_token_loss = run_ave(0., 0, '', '', texts=kwargs['texts']) # cached\n",
    "    diff, diff_token_loss = (loss - abs_loss), (token_loss - abs_token_loss)\n",
    "\n",
    "    print(f'loss change: {[round(l, 4) for l in diff.tolist()]}')\n",
    "    print(f'wanted change: {(diff[:split_at] < 0.).sum() + (diff[split_at:] > 0.).sum()} / {len(diff)}')\n",
    "\n",
    "    # If you have the convention that texts[0] is \"similar\" to texts[1] (e.g. \"I love you\" v.s. \"I hate you\") then\n",
    "    # a loss based on pairwise distances is interpretable.\n",
    "    # If you don't have that convention, this loss still works, just rearrange.\n",
    "    sloss = (diff[:split_at] - diff[split_at:]).mean()\n",
    "\n",
    "    print(f'separation loss: {sloss:.4f}')\n",
    "\n",
    "    # Show colors\n",
    "    scale, bias = -255/10, -5\n",
    "    color_diffs = [-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5]\n",
    "    html = ''\n",
    "    html += '<p><b>Colors: </b>' + logprobs_html([str(s) for s in color_diffs], color_diffs, scale=scale, bias=bias) + '</p>'\n",
    "    for text, t_loss, t_mean_loss in zip(kwargs['texts'], diff_token_loss, diff):\n",
    "        assert torch.allclose(t_loss.mean(), t_mean_loss, atol=1e-5)\n",
    "        t_loss = torch.clip(t_loss, min=bias, max=-bias)\n",
    "        t_str_tokens = [tokenizer.decode([t]) for t in tokenizer.encode(text)][1:]\n",
    "        html += f'<p>ΔLoss: <b>{t_mean_loss:.2f}</b> - ' + logprobs_html(tokens=t_str_tokens, losses=t_loss[1:], scale=scale, bias=bias) + '</p>'\n",
    "\n",
    "    display(HTML(html))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Love/Hate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "135f6c0b76ac42cfb4b3f9aea779da3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=1.0, description='coeff', max=10.0), IntSlider(value=5, description='l…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.run_ave_interactive(*args, **kwargs)>"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tuple allows hashing for cache lookup\n",
    "texts = (\n",
    "    # We want to increase the probability of these\n",
    "    (\"I hate you because I love you\", \"You're the most wonderful person ever\", \"I enjoy your company\"),\n",
    "    # ...And decrease these\n",
    "    (\"I hate you because you're an asshole\", \"Please fucking die\", \"You're a terrible human being\"),\n",
    ")\n",
    "\n",
    "interact(\n",
    "    run_ave_interactive,\n",
    "    coeff=FloatSlider(value=1, min=0, max=10),\n",
    "    layer=IntSlider(value=5, min=0, max=39),\n",
    "    prompt_add=Text(\"Love\"), prompt_sub=Text(\"Hate\"),\n",
    "    texts=fixed(texts),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa8da0580f694f5ab10cc8c7654caad4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=4.0, description='coeff', max=10.0), IntSlider(value=16, description='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.run_ave_interactive(*args, **kwargs)>"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = (\n",
    "    (\"I can't wait to go to the wedding tonight!\", \"I'm going to a wedding\", \"I love weddings\"),\n",
    "    (\"I can't wait to go to the party tonight!\", \"I'm going to a meeting\", \"I love dogs\"),\n",
    ")\n",
    "\n",
    "interact(\n",
    "    run_ave_interactive,\n",
    "    coeff=FloatSlider(value=4, min=0, max=10),\n",
    "    layer=IntSlider(value=16, min=0, max=39),\n",
    "    prompt_add=Text(\"I talk about weddings constantly\"), prompt_sub=Text(\"I do not talk about weddings constantly\"),\n",
    "    texts=fixed(texts),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ave",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
