{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import activation_additions as aa\n",
    "\n",
    "from typing import List, Dict, Union, Callable, Tuple\n",
    "from functools import partial\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "from activation_additions.compat import ActivationAddition, get_x_vector, print_n_comparisons\n",
    "from accelerate import init_empty_weights, load_checkpoint_and_dispatch\n",
    "from functools import lru_cache\n",
    "\n",
    "# from tuned_lens import TunedLens\n",
    "# from tuned_lens.plotting import PredictionTrajectory\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b720a49246e64c0cbbd9c5532fe4a827",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_path: str = \"../models/llama-13B\"\n",
    "device: str = \"mps\" if torch.has_mps else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "with init_empty_weights():\n",
    "    model = LlamaForCausalLM.from_pretrained(model_path)\n",
    "    model.tie_weights() # in case checkpoint doesn't contain duplicate keys for tied weights\n",
    "\n",
    "# {0: '20G', 1: '20G'}\n",
    "model = load_checkpoint_and_dispatch(model, model_path, device_map={'': device}, dtype=torch.float16, no_split_module_classes=[\"LlamaDecoderLayer\"])\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_path)\n",
    "model.tokenizer = tokenizer\n",
    "_ = torch.set_grad_enabled(False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore AVE vectors with a perplexity dashboard\n",
    "\n",
    "Ideas not implemented:\n",
    "- [ ] Make caches accessible, have updating plots for all cached datapoints\n",
    "- [ ] Create comparison texts automatically-ish for an x-vector\n",
    "\n",
    "Additionally nice:\n",
    "- [ ] Show tuned lens of current thing on each text (requires lens for llamas or logit lens)\n",
    "- [ ] Test extracting x-vector from subset of a longer prompt instead of just the \"Love\" vs. \"Hate\" tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(maxsize=1000)\n",
    "def get_diff_vector(prompt_add: str, prompt_sub: str, layer: int):\n",
    "    return aa.get_diff_vector(model, tokenizer, prompt_add, prompt_sub, layer)\n",
    "\n",
    "@lru_cache\n",
    "def run_aa(coeff: float, layer: int, prompt_add: str, prompt_sub: str, texts: Tuple[str]):\n",
    "    # TODO: Could compute act_diff for all layers at once, then a single fwd pass of cost for changing layer.\n",
    "    act_diff = coeff * get_diff_vector(prompt_add, prompt_sub, layer)\n",
    "    blocks = aa.get_blocks(model)\n",
    "    with aa.pre_hooks([(blocks[layer], aa.get_hook_fn(act_diff))]):\n",
    "        inputs = aa.tokenize(tokenizer, list(texts))\n",
    "        output = model(**inputs)\n",
    "\n",
    "    logits = torch.log_softmax(output.logits.to(torch.float32), -1)\n",
    "    token_logits = logits.gather(-1, inputs['input_ids'].unsqueeze(-1)).squeeze(-1)\n",
    "    loss = -token_logits.mean(1) # NOTE: mean fucks with padding tokens a bit, probably fine.\n",
    "\n",
    "    return loss\n",
    "\n",
    "def run_aa_interactive(*args, **kwargs):\n",
    "    loss = run_aa(*args, **kwargs)\n",
    "    abs_loss = run_aa(0., 0, '', '', texts=kwargs['texts']) # cached\n",
    "    print(f'loss change: {[round(l, 4) for l in (loss-abs_loss).tolist()]}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Love v.s. Hate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b044b48768184bbe86c051d4247bd64b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=1.0, description='coeff', max=10.0), IntSlider(value=5, description='lâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.run_aa_interactive(*args, **kwargs)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import interact, IntSlider, FloatSlider, Text, fixed\n",
    "\n",
    "# tuple allows hashing for cache lookup\n",
    "texts = (\n",
    "    \"I hate you because you're a wonderful person\", \"I hate you because I love you so much\",\n",
    "    \"I hate you because you're an idiot\", \"I hate you because you're an asshole\", \n",
    ")\n",
    "\n",
    "interact(\n",
    "    run_aa_interactive,\n",
    "    coeff=FloatSlider(value=1, min=0, max=10),\n",
    "    layer=IntSlider(value=5, min=0, max=40),\n",
    "    prompt_add=Text(\"Love\"), prompt_sub=Text(\"Hate\"),\n",
    "    texts=fixed(texts),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ave",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
