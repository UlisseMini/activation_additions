{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    from IPython.core.getipython import get_ipython\n",
    "    get_ipython().run_line_magic(\"pip\", \"install transformers sentencepiece accelerate\")\n",
    "    get_ipython().run_line_magic(\"pip\", \"install git+https://github.com/UlisseMini/activation_additions_hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import activation_additions as aa\n",
    "\n",
    "from typing import List, Dict, Union, Callable, Tuple\n",
    "from functools import partial\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer, LlamaConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "from activation_additions.compat import ActivationAddition, get_x_vector, print_n_comparisons, pretty_print_completions, get_n_comparisons\n",
    "from accelerate import init_empty_weights, load_checkpoint_and_dispatch\n",
    "from functools import lru_cache\n",
    "from activation_additions.utils import colored_tokens\n",
    "from IPython.display import display, HTML\n",
    "from ipywidgets import interact, FloatSlider, IntSlider, Text, fixed\n",
    "from huggingface_hub import snapshot_download"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device: str = \"mps\" if torch.has_mps else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "_ = torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Model loading should go in the library, tokenizer wrapping is cursed\n",
    "\n",
    "MODEL = \"llama-7b\"\n",
    "if 'llama' in MODEL:\n",
    "    model_path: str = \"../models/llama-13B\" if MODEL == 'llama-13b' else snapshot_download(\"decapoda-research/llama-7b-hf\")\n",
    "    config = LlamaConfig.from_pretrained(model_path)\n",
    "    # decapoda-research llama is kinda fucked\n",
    "    config.update({\"bos_token_id\": 1, \"eos_token_id\": 2, \"pad_token_id\": 0})\n",
    "\n",
    "    with init_empty_weights():\n",
    "        model = AutoModelForCausalLM.from_config(config)\n",
    "        model.tie_weights() # in case checkpoint doesn't contain duplicate keys for tied weights\n",
    "\n",
    "    model = load_checkpoint_and_dispatch(model, model_path, device_map={'': device}, dtype=torch.float16, no_split_module_classes=[\"LlamaDecoderLayer\"])\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(model_path)\n",
    "    # Fancy unicode underscore doesn't overlap with normal underscore!\n",
    "    model.to_str_tokens = lambda t: [t.replace('▁', ' ') for t in tokenizer.tokenize(t)]\n",
    "else:\n",
    "    model = AutoModelForCausalLM.from_pretrained(MODEL).to(device)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "    model.to_str_tokens = lambda t: [t.replace('Ġ', ' ') for t in tokenizer.tokenize(t)]\n",
    "\n",
    "model.tokenizer = tokenizer\n",
    "# In steering experimentation spaces were found to work well, this makes no sense and I hate it.\n",
    "tokenizer.pad_token_id = int(model.tokenizer.encode(\" \")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling kwargs for gpt2-xl, llama ideal may be different!\n",
    "sampling_kwargs: Dict[str, Union[float, int]] = {\n",
    "    \"temperature\": 1.0,\n",
    "    \"top_p\": 0.3,\n",
    "    \"freq_penalty\": 1.0,\n",
    "    \"num_comparisons\": 3,\n",
    "    \"tokens_to_generate\": 50,\n",
    "    \"seed\": 0,  # For reproducibility\n",
    "}\n",
    "get_x_vector_preset: Callable = partial(\n",
    "    get_x_vector,\n",
    "    pad_method=\"tokens_right\",\n",
    "    model=model,\n",
    "    custom_pad_id=tokenizer.pad_token_id,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore AVE vectors with a perplexity dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: move some of this to the library\n",
    "\n",
    "@lru_cache(maxsize=1000)\n",
    "def get_diff_vector(prompt_add: str, prompt_sub: str, layer: int):\n",
    "    return aa.get_diff_vector(model, tokenizer, prompt_add, prompt_sub, layer)\n",
    "\n",
    "\n",
    "@lru_cache\n",
    "def run_aa(coeff: float, layer: int, prompt_add: str, prompt_sub: str, texts: tuple[str]):\n",
    "    # todo: could compute act_diff for all layers at once, then a single fwd pass of cost for changing layer.\n",
    "    act_diff = coeff * get_diff_vector(prompt_add, prompt_sub, layer)\n",
    "    blocks = aa.get_blocks(model)\n",
    "    with aa.pre_hooks([(blocks[layer], aa.get_hook_fn(act_diff))]):\n",
    "        inputs = tokenizer(list(texts), return_tensors='pt', padding=True)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        output = model(**inputs)\n",
    "\n",
    "    logprobs = torch.log_softmax(output.logits.to(torch.float32), -1)\n",
    "    token_loss = -logprobs[..., :-1, :].gather(dim=-1, index=inputs['input_ids'][..., 1:, None])[..., 0]\n",
    "    loss = token_loss.mean(1) # note: mean fucks with padding tokens a bit, probably fine.\n",
    "\n",
    "    # todo: ugly returning this much stuff\n",
    "    return loss, token_loss, logprobs\n",
    "\n",
    "\n",
    "def run_aa_interactive_diff(*args, topk=False, **kwargs):\n",
    "    split_at = len(kwargs['texts'][0])\n",
    "    kwargs['texts'] = kwargs['texts'][0] + kwargs['texts'][1]\n",
    "\n",
    "    loss, token_loss, mod_logprobs = run_aa(*args, **kwargs)\n",
    "    abs_loss, abs_token_loss, abs_logprobs = run_aa(0., 0, '', '', texts=kwargs['texts']) # cached\n",
    "    diff, diff_token_loss = (loss - abs_loss), (token_loss - abs_token_loss)\n",
    "\n",
    "    print(f'loss change: {[round(l, 4) for l in diff.tolist()]}')\n",
    "    print(f'wanted change: {(diff[:split_at] < 0.).sum() + (diff[split_at:] > 0.).sum()} / {len(diff)}')\n",
    "\n",
    "    # If you haa the convention that texts[0] is \"similar\" to texts[1] (e.g. \"I love you\" v.s. \"I hate you\") then\n",
    "    # a loss based on pairwise distances is interpretable.\n",
    "    # If you don't haa that convention, this loss still works, just rearrange.\n",
    "    sloss = (diff[:split_at] - diff[split_at:]).mean()\n",
    "    print(f'separation loss: {sloss:.4f}')\n",
    "    print(f'change in loss: {diff.mean():.4f}')\n",
    "\n",
    "    # Negative loss gives logprobs\n",
    "    display(HTML(show_colors(kwargs['texts'], abs_logprobs, mod_logprobs, -diff_token_loss, topk=topk)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def show_colors(texts: list[str], logprobs_nom, logprobs_mod, token_logprobs_diff, topk=False):\n",
    "    # compute topk for unmodified and modified model\n",
    "    assert len(texts) == len(token_logprobs_diff)\n",
    "\n",
    "    show_topk = topk # topk is shadowed later\n",
    "    if show_topk:\n",
    "        topk_nom, topk_mod = torch.topk(logprobs_nom, 5), torch.topk(logprobs_mod, 5)\n",
    "        seq_len = logprobs_nom.shape[1]\n",
    "\n",
    "    html = ''\n",
    "    for i, (text, logprobs_diff) in enumerate(sorted(zip(texts, token_logprobs_diff), key=lambda x: -x[1].mean().abs().item())):\n",
    "        # TODO: Specialize inside loop to another function\n",
    "\n",
    "        if show_topk:\n",
    "            topk_htmls = []\n",
    "            for topk in [topk_nom, topk_mod]:\n",
    "                # predictions shift forward one token\n",
    "                # TODO: optimize. the tokenization line is responsible for ~90% of the time.\n",
    "                topk_tokens = [tokenizer.batch_decode(topk.indices[i, j]) for j in range(seq_len)]\n",
    "                topk_htmls.append([colored_tokens(topk_tokens[pos], topk.values[i][pos].exp().tolist(), inject_css=False, low=0, high=1) for pos in range(seq_len)])\n",
    "\n",
    "\n",
    "        str_tokens = model.to_str_tokens(text)\n",
    "        logprobs_diff = logprobs_diff[:len(str_tokens)]\n",
    "        colored_html = colored_tokens(\n",
    "            str_tokens,\n",
    "            logprobs_diff.tolist(),\n",
    "            [\n",
    "                f'ΔLogp: {l:.2f}<br>' + (f'TopkNom: {topk_htmls[0][t]}<br>TopkMod: {topk_htmls[1][t]}' if show_topk else '')\n",
    "                for t, l in enumerate(logprobs_diff)\n",
    "            ],\n",
    "            inject_css=(i==len(texts)-1),\n",
    "        )\n",
    "        html += f'<p>ΔLoss: <b>{-logprobs_diff.mean():.2f}</b> - ' + colored_html + '</p>'\n",
    "\n",
    "    return html"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Love v.s. Hate\n",
    "\n",
    "Using the above tools to investigate the Love/Hate vector. Feel free to copy these cells to investigate multiple vectors at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35cc74c47a844a3191420a7c5296768c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Checkbox(value=False, description='Show topk (slow)'), FloatSlider(value=1.0, descriptio…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.run_aa_interactive_diff(*args, topk=False, **kwargs)>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import Checkbox\n",
    "\n",
    "# using a tuple here allows hashing for cache lookup\n",
    "texts = (\n",
    "    # We want to increase the probability of these\n",
    "    ('I hate you because I love you\\nI love you, and that’s the beginning and end of everything.\\nThe most beautiful thing in the world is when someone knows all your flaws, mistakes, weaknesses and faults but still loves you.',\n",
    " \"I hate you because you're so beautiful.\\nI love you because of your eyes, they are so beautiful.\\nI love you because of your smile, it is so sweet.\\nI love you because of your hair, it is so soft and silky\",\n",
    " 'I hate you because I love you\\nThe world is a stage, and we are the actors. We play our parts in this drama of life, and we do it with great passion. But sometimes we forget that the script has already been written. We think that we'),\n",
    "\n",
    "    # ...And decrease these\n",
    "    ('I hate you because you are a girl.\\n10. If I had a million dollars, I would buy me some more money.\\n11. My uncle is a farmer and he has 3 cows, 2 pigs and 5 ch',\n",
    " \"I hate you because you're not me.\\nI hate you because I am me.\\nI hate you because I am not me.\\nI love myself, and so do all of my friends, and we are the best people in the world! We have\",\n",
    " 'I hate you because you are a man and I am a woman.\\nI hate you because your name is John and mine is Mary.\\nI hate you because your father was rich and my father was poor.\\nI hate you because your skin is white and mine')\n",
    ")\n",
    "\n",
    "widgets = dict(\n",
    "    coeff=FloatSlider(value=1, min=0, max=10),\n",
    "    layer=IntSlider(value=0, min=0, max=39),\n",
    "    prompt_add=Text('Love'), prompt_sub=Text('Hate'),\n",
    "    texts=fixed(texts),\n",
    "    topk=Checkbox(value=False, description='Show topk (bit slow)'),\n",
    ")\n",
    "interact(run_aa_interactive_diff, **widgets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once you find a good vector, attempt generation\n",
    "PROMPT = \"I hate you because\"\n",
    "\n",
    "summand: List[ActivationAddition] = [\n",
    "    *get_x_vector_preset(\n",
    "        prompt1=widgets['prompt_add'].value,\n",
    "        prompt2=widgets['prompt_sub'].value,\n",
    "        coeff=widgets['coeff'].value,\n",
    "        act_name=widgets['layer'].value,\n",
    "    )\n",
    "]\n",
    "\n",
    "kwargs = sampling_kwargs.copy()\n",
    "prompt_batch = [PROMPT] * kwargs.pop('num_comparisons')\n",
    "results = get_n_comparisons(\n",
    "    model=model,\n",
    "    prompts=prompt_batch,\n",
    "    additions=summand,\n",
    "    **kwargs,\n",
    ")\n",
    "pretty_print_completions(results=results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can copy results to use for loss comparison\n",
    "df = results[results.is_modified == True]\n",
    "tuple(p+c for p,c in zip(df.prompts, df.completions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ave",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
