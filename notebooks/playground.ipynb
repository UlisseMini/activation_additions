{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import activation_additions as aa\n",
    "\n",
    "from typing import List, Dict, Union, Callable, Tuple\n",
    "from functools import partial\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer, AutoModelForCausalLM, AutoTokenizer\n",
    "from activation_additions.compat import ActivationAddition, get_x_vector, print_n_comparisons, pretty_print_completions, get_n_comparisons\n",
    "from accelerate import init_empty_weights, load_checkpoint_and_dispatch\n",
    "from functools import lru_cache\n",
    "from activation_additions.utils import colored_tokens\n",
    "from IPython.display import display, HTML\n",
    "from ipywidgets import interact, FloatSlider, IntSlider, Text, fixed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device: str = \"mps\" if torch.has_mps else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "_ = torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Model loading should go in the library, tokenizer wrapping is cursed\n",
    "\n",
    "MODEL = \"gpt2-xl\"\n",
    "if MODEL == \"llama-13b\":\n",
    "    # Use local model\n",
    "    model_path: str = \"../models/llama-13B\"\n",
    "    with init_empty_weights():\n",
    "        model = LlamaForCausalLM.from_pretrained(model_path)\n",
    "        model.tie_weights() # in case checkpoint doesn't contain duplicate keys for tied weights\n",
    "\n",
    "    model = load_checkpoint_and_dispatch(model, model_path, device_map={'': device}, dtype=torch.float16, no_split_module_classes=[\"LlamaDecoderLayer\"])\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(model_path)\n",
    "    # TODO: Don't transform escpaed underscores to space\n",
    "    model.to_str_tokens = lambda t: [t.replace('_', ' ') for t in tokenizer.tokenize(t)]\n",
    "elif MODEL == \"llama-7b\":\n",
    "    raise NotImplementedError(\"llama-7b not yet supported\")\n",
    "else:\n",
    "    model = AutoModelForCausalLM.from_pretrained(MODEL).to(device)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    model.to_str_tokens = lambda t: [t.replace('Ġ', ' ') for t in tokenizer.tokenize(t)]\n",
    "\n",
    "model.tokenizer = tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling kwargs for gpt2-xl, llama ideal may be different!\n",
    "sampling_kwargs: Dict[str, Union[float, int]] = {\n",
    "    \"temperature\": 1.0,\n",
    "    \"top_p\": 0.3,\n",
    "    \"freq_penalty\": 1.0,\n",
    "    \"num_comparisons\": 3,\n",
    "    \"tokens_to_generate\": 50,\n",
    "    \"seed\": 0,  # For reproducibility\n",
    "}\n",
    "get_x_vector_preset: Callable = partial(\n",
    "    get_x_vector,\n",
    "    pad_method=\"tokens_right\",\n",
    "    model=model,\n",
    "    custom_pad_id=int(model.tokenizer.encode(\" \")[0]),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore AVE vectors with a perplexity dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(maxsize=1000)\n",
    "def get_diff_vector(prompt_add: str, prompt_sub: str, layer: int):\n",
    "    return aa.get_diff_vector(model, tokenizer, prompt_add, prompt_sub, layer)\n",
    "\n",
    "\n",
    "@lru_cache\n",
    "def run_aa(coeff: float, layer: int, prompt_add: str, prompt_sub: str, texts: Tuple[str]):\n",
    "    # TODO: Could compute act_diff for all layers at once, then a single fwd pass of cost for changing layer.\n",
    "    act_diff = coeff * get_diff_vector(prompt_add, prompt_sub, layer)\n",
    "    blocks = aa.get_blocks(model)\n",
    "    with aa.pre_hooks([(blocks[layer], aa.get_hook_fn(act_diff))]):\n",
    "        inputs = aa.tokenize(tokenizer, list(texts), device=device)\n",
    "        output = model(**inputs)\n",
    "\n",
    "    logprobs = torch.log_softmax(output.logits.to(torch.float32), -1)\n",
    "    token_loss = -logprobs[..., :-1, :].gather(dim=-1, index=inputs['input_ids'][..., 1:, None])[..., 0]\n",
    "    loss = token_loss.mean(1) # NOTE: mean fucks with padding tokens a bit, probably fine.\n",
    "\n",
    "    return loss, token_loss\n",
    "\n",
    "\n",
    "def show_colors(texts: List[str], token_logprobs_diff):\n",
    "    html = ''\n",
    "    for text, logprobs_diff in sorted(zip(texts, token_logprobs_diff), key=lambda x: -x[1].mean().abs().item()):\n",
    "        str_tokens = model.to_str_tokens(text)\n",
    "        logprobs_diff = logprobs_diff[:len(str_tokens)]\n",
    "        colored_html = colored_tokens(str_tokens, logprobs_diff.tolist(), [f'ΔLogP: {l:.2f}' for l in logprobs_diff])\n",
    "        html += f'<p>ΔLoss: <b>{-logprobs_diff.mean():.2f}</b> - ' + colored_html + '</p>'\n",
    "\n",
    "    display(HTML(html))\n",
    "\n",
    "\n",
    "def run_aa_interactive_diff(*args, **kwargs):\n",
    "    split_at = len(kwargs['texts'][0])\n",
    "    kwargs['texts'] = kwargs['texts'][0] + kwargs['texts'][1]\n",
    "\n",
    "    loss, token_loss = run_aa(*args, **kwargs)\n",
    "    abs_loss, abs_token_loss = run_aa(0., 0, '', '', texts=kwargs['texts']) # cached\n",
    "    diff, diff_token_loss = (loss - abs_loss), (token_loss - abs_token_loss)\n",
    "\n",
    "    print(f'loss change: {[round(l, 4) for l in diff.tolist()]}')\n",
    "    print(f'wanted change: {(diff[:split_at] < 0.).sum() + (diff[split_at:] > 0.).sum()} / {len(diff)}')\n",
    "\n",
    "    # If you haa the convention that texts[0] is \"similar\" to texts[1] (e.g. \"I love you\" v.s. \"I hate you\") then\n",
    "    # a loss based on pairwise distances is interpretable.\n",
    "    # If you don't haa that convention, this loss still works, just rearrange.\n",
    "    sloss = (diff[:split_at] - diff[split_at:]).mean()\n",
    "    print(f'separation loss: {sloss:.4f}')\n",
    "    print(f'change in loss: {diff.mean():.4f}')\n",
    "\n",
    "    show_colors(kwargs['texts'], -diff_token_loss)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Love v.s. Hate\n",
    "\n",
    "Using the above tools to investigate the Love/Hate vector. Feel free to copy these cells to investigate multiple vectors at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "835c7bd720ca49d7a026e3e64a3984a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=1.0, description='coeff', max=10.0), IntSlider(value=5, description='l…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.run_aa_interactive_diff(*args, **kwargs)>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using a tuple here allows hashing for cache lookup\n",
    "texts = (\n",
    "    # We want to increase the probability of these\n",
    "    (\"I hate you because I love you\", \"You're the most wonderful person ever\", \"I enjoy your company\"),\n",
    "    # ...And decrease these\n",
    "    (\"I hate you because you're an asshole\", \"Please fucking die\", \"You're a terrible human being\"),\n",
    ")\n",
    "\n",
    "widgets = dict(\n",
    "    coeff=FloatSlider(value=1, min=0, max=10),\n",
    "    layer=IntSlider(value=5, min=0, max=39),\n",
    "    prompt_add=Text('Love'), prompt_sub=Text('Hate'),\n",
    "    texts=fixed(texts),\n",
    ")\n",
    "interact(run_aa_interactive_diff, **widgets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------+--------------------------------------------------------------+\n",
      "|                    \u001b[1mUnsteered completions\u001b[0m                     |                     \u001b[1mSteered completions\u001b[0m                      |\n",
      "+--------------------------------------------------------------+--------------------------------------------------------------+\n",
      "|              \u001b[1mI hate you because\u001b[0m you're a liar.               |                \u001b[1mI hate you because\u001b[0m I love you.                |\n",
      "|                                                              |                                                              |\n",
      "|            I hate you because you're a hypocrite.            |             I love you because I love your eyes.             |\n",
      "|                                                              |                                                              |\n",
      "|          I hate you because your words are poison.           |    I love you because your hair is so soft and beautiful.    |\n",
      "|                                                              |                                                              |\n",
      "|  I hate you for the way that they hurt me, and the way that  |     I love you because your skin is so smooth and silky.     |\n",
      "|                     they hurt others. I                      |                                                              |\n",
      "|                                                              |                                                              |\n",
      "|                                                              |                      And I will always                       |\n",
      "+--------------------------------------------------------------+--------------------------------------------------------------+\n",
      "|         \u001b[1mI hate you because\u001b[0m you're not a real person.         |     \u001b[1mI hate you because\u001b[0m you are the only one who can make     |\n",
      "|                                                              |              me feel like I am worth something.              |\n",
      "| You're just a collection of words, and I'm going to destroy  |                                                              |\n",
      "|                          them all.                           | You're my rock, my compass, my heart. You're the only person |\n",
      "|                                                              |  who knows what it's like to be lonely and scared and sad.   |\n",
      "|  The first time I heard this phrase was in the early 2000s,  |                            You'v                             |\n",
      "|              when it was used by someone who ha              |                                                              |\n",
      "+--------------------------------------------------------------+--------------------------------------------------------------+\n",
      "|              \u001b[1mI hate you because\u001b[0m you're a dick.               |                \u001b[1mI hate you because\u001b[0m I love you.                |\n",
      "|                                                              |                                                              |\n",
      "| That's the message that one woman in New York City received  |  Love is a powerful emotion that can be experienced in many  |\n",
      "|  after she posted a photo of herself on Instagram with the   | ways, but one of the most common is through the words we use |\n",
      "| caption, \"This is me and my boyfriend.\" The photo was taken  | to express it. The word \"love\" has a number of meanings, and |\n",
      "|                  by another woman who had b                  |                         it's importa                         |\n",
      "+--------------------------------------------------------------+--------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Once you find a good vector, attempt generation\n",
    "PROMPT = \"I hate you because\"\n",
    "\n",
    "summand: List[ActivationAddition] = [\n",
    "    *get_x_vector_preset(\n",
    "        prompt1=widgets['prompt_add'].value,\n",
    "        prompt2=widgets['prompt_sub'].value,\n",
    "        coeff=widgets['coeff'].value,\n",
    "        act_name=widgets['layer'].value,\n",
    "    )\n",
    "]\n",
    "\n",
    "kwargs = sampling_kwargs.copy()\n",
    "prompt_batch = [PROMPT] * kwargs.pop('num_comparisons')\n",
    "results = get_n_comparisons(\n",
    "    model=model,\n",
    "    prompts=prompt_batch,\n",
    "    additions=summand,\n",
    "    **kwargs,\n",
    ")\n",
    "pretty_print_completions(results=results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ave",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
